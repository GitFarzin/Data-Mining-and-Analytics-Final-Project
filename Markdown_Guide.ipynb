{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured pypdf python-docx nltk --quiet  # Install Unstructured to extract structured content from PDFs\n",
        "!pip install pdfminer.six --quiet\n",
        "!pip install \"unstructured[pdf]\" --quiet\n",
        "# Download NLTK tokenizer for sentence splitting (used later)\n",
        "import nltk\n",
        "nltk.download(\"punkt\")# Required for sentence tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2IFZ0MH2RBq",
        "outputId": "e972eeac-fca6-4c3c-d539-aa79f171ceb7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and Load PDF\n",
        "\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import os\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "# Place path for PDF application\n",
        "pdf_path = \"/content/NoMarkUP30%_LA_Ch1_General Info_FINAL.pdf\"\n",
        "\n",
        "# Use Unstructured's partition_pdf to break the PDF into structured elements\n",
        "elements = partition_pdf(filename=pdf_path)\n",
        "\n",
        "# Preview total elements found\n",
        "print(f\"Total elements extracted: {len(elements)}\")\n",
        "print(\"First element preview:\")\n",
        "print(elements[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HavqKql6DDs5",
        "outputId": "2d063d08-4da2-4077-9e50-285e35162d3b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elements extracted: 1099\n",
            "First element preview:\n",
            "Eagle RockIKE Enrichment Facility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Need to return and categorize table of content entries\n"
      ],
      "metadata": {
        "id": "XWgGiJN5b5El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Collect all short blocks from the document\n",
        "# We're focusing on blocks with <10 words — likely to be headers, footers, or labels\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "short_blocks = [\n",
        "    el.text.strip()\n",
        "    for el in elements\n",
        "    if hasattr(el, \"text\") and 0 < len(el.text.strip().split()) < 10\n",
        "]\n",
        "\n",
        "#  Count how often each short block appears\n",
        "footer_counts = Counter(short_blocks)\n",
        "\n",
        "# If something appears too often (e.g., >5 times), it's probably a footer/header\n",
        "# You can tune this threshold depending on document size\n",
        "common_footers = {\n",
        "    text for text, count in footer_counts.items() if count > 5\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "ddS9Oi44IlEZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Define a helper function to detect bullet points\n",
        "def is_bullet_point(text):\n",
        "    \"\"\"\n",
        "    Returns True if a line looks like a bullet point.\n",
        "    Matches:\n",
        "      - • Conduct...\n",
        "      - - Maintain...\n",
        "      - * Submit...\n",
        "      - 1. Evaluate...\n",
        "      - 2) Review...\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^(\\s*[\\*\\-•]\\s+|\\d+[\\.\\)]\\s+)\", text))\n",
        "\n",
        "\n",
        "# Step 2: Filter out unwanted text\n",
        "# Keep text blocks if they are either:\n",
        "# - Long paragraphs (>15 words), OR\n",
        "# - Bullet points (even short ones)\n",
        "\n",
        "filtered_chunks = []\n",
        "discarded_chunks = []\n",
        "\n",
        "for el in elements:\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        word_count = len(text.split())\n",
        "\n",
        "        if word_count > 15 or is_bullet_point(text):\n",
        "            filtered_chunks.append(text)\n",
        "        else:\n",
        "            discarded_chunks.append(text)\n",
        "\n",
        "# ---- Step 3: Show basic stats ----\n",
        "print(f\" Retained: {len(filtered_chunks)} meaningful paragraphs\")\n",
        "print(f\" Discarded: {len(discarded_chunks)} short or noisy blocks\")\n",
        "\n",
        "# Optional: Preview a few filtered results\n",
        "print(\"\\n Sample Clean Paragraph:\\n\")\n",
        "print(filtered_chunks[0][:500])\n",
        "\n",
        "# Optional: Preview what was discarded (debugging)\n",
        "print(\"\\n Sample Discarded Text:\\n\")\n",
        "print(discarded_chunks[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1olMyz2gc0",
        "outputId": "895ab242-c9ca-4193-9732-82c6fb2bb00f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# --- Step 1: Extract TOC entries from top of document ---\n",
        "toc_entries = []\n",
        "toc_pattern = re.compile(r\"^(\\d+(\\.\\d+)+)\\s+(.+?)\\.{3,}\\s+(\\S+)$\")\n",
        "\n",
        "for i, el in enumerate(elements[:150]):\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        match = toc_pattern.match(text)\n",
        "        if match:\n",
        "            section = match.group(1)\n",
        "            title = match.group(3).strip()\n",
        "            page = match.group(4).strip()\n",
        "            toc_entries.append({\"section\": section, \"title\": title, \"toc_page\": page})\n",
        "\n",
        "# --- Step 2: Scan entire doc for actual body sections ---\n",
        "body_sections = []\n",
        "header_pattern = re.compile(r\"^(\\d+(\\.\\d+)+)\\s+(.+)$\")\n",
        "\n",
        "for i, el in enumerate(elements):\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        match = header_pattern.match(text)\n",
        "        if match:\n",
        "            section = match.group(1)\n",
        "            title = match.group(3).strip()\n",
        "            body_sections.append({\"section\": section, \"title\": title, \"index\": i})\n",
        "\n",
        "# --- Step 3: Create DocMap ---\n",
        "doc_map = {}\n",
        "\n",
        "for idx, entry in enumerate(body_sections):\n",
        "    section = entry[\"section\"]\n",
        "    doc_map[section] = {\n",
        "        \"title\": entry[\"title\"],\n",
        "        \"start_index\": entry[\"index\"],\n",
        "        \"end_index\": (\n",
        "            body_sections[idx + 1][\"index\"] - 1\n",
        "            if idx + 1 < len(body_sections)\n",
        "            else len(elements) - 1\n",
        "        )\n",
        "    }\n",
        "\n",
        "# Optional: Add TOC page numbers\n",
        "for toc in toc_entries:\n",
        "    if toc[\"section\"] in doc_map:\n",
        "        doc_map[toc[\"section\"]][\"toc_page\"] = toc[\"toc_page\"]\n",
        "\n",
        "# --- Preview ---\n",
        "import pprint\n",
        "pprint.pprint(dict(list(doc_map.items())[:5]))\n"
      ],
      "metadata": {
        "id": "FF_rS1qegdQQ",
        "outputId": "2922aa9e-5720-4ad0-e24f-4d3c3b0c94ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1.1.1': {'end_index': 34,\n",
            "           'start_index': 34,\n",
            "           'title': 'Facility Location, Site Layout, and Surrounding '\n",
            "                    'Characteristics ......... 1.1-3',\n",
            "           'toc_page': '1.1-3'},\n",
            " '1.1.2': {'end_index': 35,\n",
            "           'start_index': 35,\n",
            "           'title': 'Facilities Description '\n",
            "                    '......................................................................... '\n",
            "                    '1.1-3',\n",
            "           'toc_page': '1.1-3'},\n",
            " '1.1.3': {'end_index': 38,\n",
            "           'start_index': 36,\n",
            "           'title': 'Process Descriptions '\n",
            "                    '......................................................................... '\n",
            "                    '1.1-7',\n",
            "           'toc_page': '1.1-7'},\n",
            " '1.1.4': {'end_index': 12,\n",
            "           'start_index': 10,\n",
            "           'title': 'Descriptive Summary of Licensed Material '\n",
            "                    '...................................... 1.1-16',\n",
            "           'toc_page': '1.1-16'},\n",
            " '1.2.1': {'end_index': 39,\n",
            "           'start_index': 39,\n",
            "           'title': 'Corporate Identity '\n",
            "                    '.............................................................................. '\n",
            "                    '1.2-1',\n",
            "           'toc_page': '1.2-1'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Save cleaned output to file (optional) ===\n",
        "with open(\"clean_application.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for para in filtered_chunks:\n",
        "        f.write(para + \"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NuJTMm5eF558"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Utility Code: Extract full text for any section ===\n",
        "def get_section_text(section_number, elements, doc_map):\n",
        "    \"\"\"Return full paragraph text for a given section number using doc_map.\"\"\"\n",
        "    s = doc_map[section_number][\"start_index\"]\n",
        "    e = doc_map[section_number][\"end_index\"]\n",
        "    return \"\\n\".join(\n",
        "        el.text.strip() for el in elements[s:e+1]\n",
        "        if hasattr(el, \"text\") and len(el.text.strip()) > 0\n",
        "    )\n",
        "\n",
        "# ✅ Example usage:\n",
        "print(get_section_text(\"1.1.3\", elements, doc_map)[:1000])  # Preview first 1000 characters of Section 1.1.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRkFfL3RF6CV",
        "outputId": "7d5bfea5-6bb5-48c2-ba1d-34935a23e376"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1.3 Process Descriptions ......................................................................... 1.1-7\n",
            "1.2\n",
            "INSTITUTIONAL INFORMATION .................................................................. 1.2-1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}