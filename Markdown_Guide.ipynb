{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1: Setup & Ingestion Liscence Application"
      ],
      "metadata": {
        "id": "mm3_5dYyXZBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured pypdf python-docx nltk --quiet  # Install Unstructured to extract structured content from PDFs\n",
        "!pip install pdfminer.six --quiet\n",
        "!pip install \"unstructured[pdf]\" --quiet\n",
        "# Download NLTK tokenizer for sentence splitting (used later)\n",
        "import nltk\n",
        "nltk.download(\"punkt\")# Required for sentence tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2IFZ0MH2RBq",
        "outputId": "c36693d3-babd-47b3-edb0-2bbcab446124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and Load PDF\n",
        "\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import os\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "# Place path for PDF application\n",
        "pdf_path = \"/content/NoMarkUP30%_LA_Ch1_General Info_FINAdsL.pdf\"\n",
        "\n",
        "# Use Unstructured's partition_pdf to break the PDF into structured elements\n",
        "elements = partition_pdf(filename=pdf_path)\n",
        "\n",
        "# Preview total elements found\n",
        "print(f\"Total elements extracted: {len(elements)}\")\n",
        "print(\"First element preview:\")\n",
        "print(elements[0])\n",
        "\n",
        "\n",
        "for i, el in enumerate(elements[:10]):                   #########Running into a ton of uncategorized text\n",
        "    print(f\"\\n=== Element {i} ===\")\n",
        "    print(f\"Category: {el.category}\")\n",
        "    print(f\"Text:\\n{el.text.strip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HavqKql6DDs5",
        "outputId": "417ec4f4-93a3-4b81-9319-f7e30b4b36a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elements extracted: 918\n",
            "First element preview:\n",
            "IKE Enrichment Facility\n",
            "\n",
            "=== Element 0 ===\n",
            "Category: Title\n",
            "Text:\n",
            "IKE Enrichment Facility\n",
            "\n",
            "=== Element 1 ===\n",
            "Category: Title\n",
            "Text:\n",
            "License Application\n",
            "\n",
            "=== Element 2 ===\n",
            "Category: Title\n",
            "Text:\n",
            "TABLE OF CONTENTS\n",
            "\n",
            "=== Element 3 ===\n",
            "Category: Title\n",
            "Text:\n",
            "Page\n",
            "\n",
            "=== Element 4 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "1.0\n",
            "\n",
            "=== Element 5 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "GENERAL INFORMATION ............................................................................................. 1\n",
            "\n",
            "=== Element 6 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "1.1\n",
            "\n",
            "=== Element 7 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "FACILITY AND PROCESS OVERVIEW ........................................................ 1.1-1\n",
            "\n",
            "=== Element 8 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "1.1.1 Facility Layout Description ................................................................. 1.1-3\n",
            "\n",
            "=== Element 9 ===\n",
            "Category: UncategorizedText\n",
            "Text:\n",
            "1.1.2 Process Overview ............................................................................. 1.1-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2:  Structure Detection & Mapping && Embedding & Vector Search"
      ],
      "metadata": {
        "id": "lRKjRMWmXf6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Need to return and categorize table of content entries\n"
      ],
      "metadata": {
        "id": "XWgGiJN5b5El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Collect all short blocks from the document\n",
        "# We're focusing on blocks with <10 words — likely to be headers, footers, or labels  -- THough this is likely an error and will need to be shortened or readdressed in a new way\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "short_blocks = [\n",
        "    el.text.strip()\n",
        "    for el in elements\n",
        "    if hasattr(el, \"text\") and 0 < len(el.text.strip().split()) < 10\n",
        "]\n",
        "\n",
        "#  Count how often each short block appears\n",
        "footer_counts = Counter(short_blocks)\n",
        "\n",
        "# If something appears too often (e.g., >5 times), it's probably a footer/header\n",
        "# You can tune this threshold depending on document size\n",
        "common_footers = {\n",
        "    text for text, count in footer_counts.items() if count > 5\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "ddS9Oi44IlEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Define a helper function to detect bullet points\n",
        "def is_bullet_point(text):\n",
        "    \"\"\"\n",
        "    Returns True if a line looks like a bullet point.\n",
        "    Matches:\n",
        "      - • Conduct...\n",
        "      - - Maintain...\n",
        "      - * Submit...\n",
        "      - 1. Evaluate...\n",
        "      - 2) Review...\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^(\\s*[\\*\\-•]\\s+|\\d+[\\.\\)]\\s+)\", text))\n",
        "\n",
        "\n",
        "# Step 2: Filter out unwanted text\n",
        "# Keep text blocks if they are either:\n",
        "# - Long paragraphs (>15 words), OR\n",
        "# - Bullet points (even short ones)\n",
        "\n",
        "filtered_chunks = []\n",
        "discarded_chunks = []\n",
        "\n",
        "for el in elements:\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        word_count = len(text.split())\n",
        "\n",
        "        if word_count > 15 or is_bullet_point(text):\n",
        "            filtered_chunks.append(text)\n",
        "        else:\n",
        "            discarded_chunks.append(text)\n",
        "\n",
        "# ---- Step 3: Show basic stats ----\n",
        "print(f\" Retained: {len(filtered_chunks)} meaningful paragraphs\")\n",
        "print(f\" Discarded: {len(discarded_chunks)} short or noisy blocks\")\n",
        "\n",
        "# Optional: Preview a few filtered results\n",
        "print(\"\\n Sample Clean Paragraph:\\n\")\n",
        "print(filtered_chunks[0][:500])\n",
        "\n",
        "# Optional: Preview what was discarded (debugging)\n",
        "print(\"\\n Sample Discarded Text:\\n\")\n",
        "print(discarded_chunks[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1olMyz2gc0",
        "outputId": "895ab242-c9ca-4193-9732-82c6fb2bb00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# --- Step 1: Extract TOC entries from top of document ---\n",
        "toc_entries = []\n",
        "toc_pattern = re.compile(r\"^(\\d+(\\.\\d+)+)\\s+(.+?)\\.{3,}\\s+(\\S+)$\")\n",
        "\n",
        "for i, el in enumerate(elements[:150]):\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        match = toc_pattern.match(text)\n",
        "        if match:\n",
        "            section = match.group(1)\n",
        "            title = match.group(3).strip()\n",
        "            page = match.group(4).strip()\n",
        "            toc_entries.append({\"section\": section, \"title\": title, \"toc_page\": page})\n",
        "\n",
        "# --- Step 2: Scan entire doc for actual body sections ---\n",
        "body_sections = []\n",
        "header_pattern = re.compile(r\"^(\\d+(\\.\\d+)+)\\s+(.+)$\")\n",
        "\n",
        "for i, el in enumerate(elements):\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        match = header_pattern.match(text)\n",
        "        if match:\n",
        "            section = match.group(1)\n",
        "            title = match.group(3).strip()\n",
        "            body_sections.append({\"section\": section, \"title\": title, \"index\": i})\n",
        "\n",
        "# --- Step 3: Create DocMap ---\n",
        "doc_map = {}\n",
        "\n",
        "for idx, entry in enumerate(body_sections):\n",
        "    section = entry[\"section\"]\n",
        "    doc_map[section] = {\n",
        "        \"title\": entry[\"title\"],\n",
        "        \"start_index\": entry[\"index\"],\n",
        "        \"end_index\": (\n",
        "            body_sections[idx + 1][\"index\"] - 1\n",
        "            if idx + 1 < len(body_sections)\n",
        "            else len(elements) - 1\n",
        "        )\n",
        "    }\n",
        "\n",
        "# Merge TOC page references into doc_map\n",
        "for toc in toc_entries:\n",
        "    if toc[\"section\"] in doc_map:\n",
        "        doc_map[toc[\"section\"]][\"toc_page\"] = toc[\"toc_page\"]\n",
        "\n",
        "# --- Preview ---\n",
        "import pprint\n",
        "pprint.pprint(dict(list(doc_map.items())[:5]))\n"
      ],
      "metadata": {
        "id": "FF_rS1qegdQQ",
        "outputId": "2922aa9e-5720-4ad0-e24f-4d3c3b0c94ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1.1.1': {'end_index': 34,\n",
            "           'start_index': 34,\n",
            "           'title': 'Facility Location, Site Layout, and Surrounding '\n",
            "                    'Characteristics ......... 1.1-3',\n",
            "           'toc_page': '1.1-3'},\n",
            " '1.1.2': {'end_index': 35,\n",
            "           'start_index': 35,\n",
            "           'title': 'Facilities Description '\n",
            "                    '......................................................................... '\n",
            "                    '1.1-3',\n",
            "           'toc_page': '1.1-3'},\n",
            " '1.1.3': {'end_index': 38,\n",
            "           'start_index': 36,\n",
            "           'title': 'Process Descriptions '\n",
            "                    '......................................................................... '\n",
            "                    '1.1-7',\n",
            "           'toc_page': '1.1-7'},\n",
            " '1.1.4': {'end_index': 12,\n",
            "           'start_index': 10,\n",
            "           'title': 'Descriptive Summary of Licensed Material '\n",
            "                    '...................................... 1.1-16',\n",
            "           'toc_page': '1.1-16'},\n",
            " '1.2.1': {'end_index': 39,\n",
            "           'start_index': 39,\n",
            "           'title': 'Corporate Identity '\n",
            "                    '.............................................................................. '\n",
            "                    '1.2-1',\n",
            "           'toc_page': '1.2-1'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### For tables and figure mapping\n",
        "\n",
        "table_figure_entries = []\n",
        "\n",
        "caption_pattern = re.compile(r\"^(Table|Figure)\\s+\\d+[\\.\\d\\-]*\\s+(.*)\", re.IGNORECASE)\n",
        "\n",
        "for i, el in enumerate(elements):\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        if caption_pattern.match(text):\n",
        "            table_figure_entries.append({\n",
        "                \"index\": i,\n",
        "                \"text\": text\n",
        "            })\n",
        "\n",
        "print(f\"✅ Found {len(table_figure_entries)} tables/figures\")\n",
        "for t in table_figure_entries[:5]:\n",
        "    print(t)\n"
      ],
      "metadata": {
        "id": "Nt8PpOn-DnBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# === Save cleaned output to file (optional) ===\n",
        "with open(\"clean_application.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for para in filtered_chunks:\n",
        "        f.write(para + \"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NuJTMm5eF558"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Save code to JSON\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"docmap.json\", \"w\") as f:\n",
        "    json.dump(doc_map, f, indent=2)\n"
      ],
      "metadata": {
        "id": "60tOowGND9a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Utility Code: Extract full text for any section ===\n",
        "def get_section_text(section_number, elements, doc_map):\n",
        "    \"\"\"Return full paragraph text for a given section number using doc_map.\"\"\"\n",
        "    s = doc_map[section_number][\"start_index\"]\n",
        "    e = doc_map[section_number][\"end_index\"]\n",
        "    return \"\\n\".join(\n",
        "        el.text.strip() for el in elements[s:e+1]\n",
        "        if hasattr(el, \"text\") and len(el.text.strip()) > 0\n",
        "    )\n",
        "\n",
        "# ✅ Example usage:\n",
        "print(get_section_text(\"1.1.3\", elements, doc_map)[:1000])  # Preview first 1000 characters of Section 1.1.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRkFfL3RF6CV",
        "outputId": "7d5bfea5-6bb5-48c2-ba1d-34935a23e376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1.3 Process Descriptions ......................................................................... 1.1-7\n",
            "1.2\n",
            "INSTITUTIONAL INFORMATION .................................................................. 1.2-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3: Annotation, Entity Mapping, & Semantic Consistency Checks"
      ],
      "metadata": {
        "id": "4c254nTAW9ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "OIX8kzGDXTAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "k3ij9tUrYIfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Process each cleaned paragraph\n",
        "\n",
        "\n",
        "ner_results = []\n",
        "\n",
        "for para in filtered_chunks:\n",
        "    doc = nlp(para)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\n",
        "            \"text\": ent.text,\n",
        "            \"label\": ent.label_\n",
        "        })\n",
        "    ner_results.append({\n",
        "        \"paragraph\": para,\n",
        "        \"entities\": entities\n",
        "    })\n",
        "\n",
        "print(f\" Processed {len(ner_results)} paragraphs with NER\")\n"
      ],
      "metadata": {
        "id": "Y4KBbpDFYKMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see\n",
        "\n",
        "pprint(ner_results[0])\n",
        "\n",
        "print(\"space\")\n",
        "pprint(ner_results[:2])"
      ],
      "metadata": {
        "id": "i2kHqH50YKHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Global Reference table\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dictionary of label -> set of unique entities\n",
        "global_reference_table = defaultdict(set)\n",
        "\n",
        "# Loop over all items and collect entities\n",
        "for item in ner_results:\n",
        "    for ent in item[\"entities\"]:\n",
        "        label = ent.get(\"label\") or ent.get(\"label_\")\n",
        "        text = ent[\"text\"]\n",
        "        global_reference_table[label].add(text)\n",
        "\n",
        "# Convert sets to sorted lists for easier viewing\n",
        "global_reference_table = {k: sorted(list(v)) for k, v in global_reference_table.items()}\n",
        "\n",
        "# Display\n",
        "print(\"Global Reference Table of Entities:\")\n",
        "for label, entries in global_reference_table.items():\n",
        "    print(f\"\\n{label}:\")\n",
        "    for e in entries:\n",
        "        print(f\" - {e}\")\n"
      ],
      "metadata": {
        "id": "X_UYC6BHYJI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse dates and times\n",
        "from dateutil.parser import parse as date_parse\n",
        "from dateutil.parser import ParserError\n",
        "\n",
        "normalized_dates = []\n",
        "\n",
        "# Loop over all items and process DATE entities\n",
        "for item in ner_results:\n",
        "    para = item[\"paragraph\"]\n",
        "    for ent in item[\"entities\"]:\n",
        "        label = ent.get(\"label\") or ent.get(\"label_\")\n",
        "        text = ent[\"text\"]\n",
        "\n",
        "        if label == \"DATE\":\n",
        "            try:\n",
        "                dt = date_parse(text, fuzzy=True)\n",
        "                normalized_dates.append({\n",
        "                    \"original\": text,\n",
        "                    \"standardized\": dt.isoformat(),\n",
        "                    \"paragraph\": para[:100]\n",
        "                })\n",
        "            except (ParserError, ValueError):\n",
        "                # Skip unparseable dates\n",
        "                pass\n",
        "\n",
        "# Display\n",
        "print(f\"\\n Parsed {len(normalized_dates)} dates.\")\n",
        "for d in normalized_dates:\n",
        "    print(f\"- '{d['original']}' standardized to {d['standardized']} (excerpt: '{d['paragraph']}...')\")\n"
      ],
      "metadata": {
        "id": "L89DJztObJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(normalized_dates[:3])"
      ],
      "metadata": {
        "id": "eZOxUWOpbPVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "##\n",
        "## Acronym Library\n",
        "\n",
        "\n",
        "\n",
        "acronym_dict = {\n",
        "    \"ERIEF\": \"Eagle Rock Enrichment Facility\",\n",
        "    \"SAR\": \"Safety Analysis Report\",\n",
        "    \"UF6\": \"Uranium Hexafluoride\",\n",
        "    \"NRC\": \"Nuclear Regulatory Commission\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DXC5ZYP2d_P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Create a long text /acronym replacement version of each paragraph:\n",
        "\n",
        "normalized_paragraphs = []\n",
        "\n",
        "for para in filtered_chunks:\n",
        "    norm_para = para\n",
        "    for short, long in acronym_dict.items():\n",
        "        pattern = re.compile(rf\"\\b{short}\\b\")\n",
        "        norm_para = pattern.sub(long, norm_para)\n",
        "    normalized_paragraphs.append(norm_para)\n",
        "\n",
        "print(\" Acronym normalization complete.\")\n"
      ],
      "metadata": {
        "id": "vK42nT_YeDxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare orig vs normal\n",
        "\n",
        "print(\"Original:\")\n",
        "print(filtered_chunks[0][:200])\n",
        "\n",
        "print(\"\\nNormalized:\")\n",
        "print(normalized_paragraphs[0][:200])\n"
      ],
      "metadata": {
        "id": "kd0g7tTeeKDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yyC3KKN9eynF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#refernce map for cross ref questions - see table 2.3 appendix a\n",
        "\n",
        "reference_pattern = re.compile(r\"(Table|Figure|Appendix)\\s+([\\w\\d\\.\\-]+)\", re.IGNORECASE)\n",
        "\n",
        "cross_references = []\n",
        "\n",
        "for i, para in enumerate(filtered_chunks):\n",
        "    matches = reference_pattern.findall(para)\n",
        "    if matches:\n",
        "        refs = []\n",
        "        for m in matches:\n",
        "            refs.append({\n",
        "                \"type\": m[0],\n",
        "                \"ref\": m[1]\n",
        "            })\n",
        "        cross_references.append({\n",
        "            \"paragraph_index\": i,\n",
        "            \"references\": refs\n",
        "        })\n",
        "\n",
        "print(f\"✅ Found {len(cross_references)} cross-reference mentions\")\n"
      ],
      "metadata": {
        "id": "fCf9ZdWDdrNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Verify Doc Map is WOrking correctly\n",
        "# Extract numeric parts for sorting\n",
        "def section_key(s):\n",
        "    return [int(part) for part in s.split(\".\")]\n",
        "\n",
        "# Sort sections\n",
        "sorted_sections = sorted(doc_map.keys(), key=section_key)\n",
        "\n",
        "# Check order and print\n",
        "print(\"✅ Section order verification:\")\n",
        "for s in sorted_sections:\n",
        "    print(f\"{s} - {doc_map[s]['title']}\")\n"
      ],
      "metadata": {
        "id": "Wskitor0dwsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 4:  Ingest and Chunk NUREG Document"
      ],
      "metadata": {
        "id": "3e1o5Dx7ezu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured pdfminer.six nltk --quiet\n"
      ],
      "metadata": {
        "id": "WqWPaB4zA6ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nureg_path = \"/content/NUREG-1520.txt\"  # Change to your filepath\n",
        "\n",
        "with open(nureg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nureg_text = f.read()\n",
        "\n",
        "print(f\"✅ NUREG loaded: {len(nureg_text):,} characters\")\n"
      ],
      "metadata": {
        "id": "pi-S5cX9Aelk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "pdf_path = \"/content/NUREG-1520.pdf\"  # Update path\n",
        "elements = partition_pdf(filename=pdf_path)\n",
        "\n",
        "# Extract text\n",
        "nureg_text = \"\\n\\n\".join([el.text for el in elements if hasattr(el, \"text\")])\n",
        "print(f\"✅ PDF parsed into {len(elements)} elements\")\n"
      ],
      "metadata": {
        "id": "pHIIUf_XAjIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######\n",
        "###### CHUNKing\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,        # ~700–1000 words max\n",
        "    chunk_overlap=150,      # for better context\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # fallback breaks\n",
        ")\n",
        "\n",
        "nureg_chunks = splitter.split_text(nureg_text)\n",
        "\n",
        "print(f\"✅ Total NUREG Chunks: {len(nureg_chunks)}\")\n",
        "print(\"\\n🔹 Sample Chunk:\\n\", nureg_chunks[0][:500])\n"
      ],
      "metadata": {
        "id": "AFrN-0LcAkpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create Semantic Vector Store"
      ],
      "metadata": {
        "id": "OqbuwjNLBBkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers langchain --quiet\n"
      ],
      "metadata": {
        "id": "mZQEc4dbBHXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings  ## LANGCHAINNN\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "mxGutqOTBMuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "## Build a Facebook AI Similarity Search (FAISS) Vector Store\n",
        "\n",
        "## look at chromadb\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Build vector index from the NUREG chunks\n",
        "vectorstore = FAISS.from_texts(nureg_chunks, embedding_model)\n",
        "\n",
        "print(\"✅ Vector store built.\")\n"
      ],
      "metadata": {
        "id": "0CQz_tAKBRxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.save_local(\"nureg_faiss_index\")   ## Come back\n",
        "vectorstore = FAISS.load_local(\"nureg_faiss_index\", embedding_model)  # must pass the same embedding model used for saving,\n",
        "#or else queries might not match as expected.\n",
        "\n",
        "# Needs  index.pkl) to the \"nureg_faiss_index\" folder on your local disk  .. file?\n"
      ],
      "metadata": {
        "id": "G9NGVOuTCH8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}