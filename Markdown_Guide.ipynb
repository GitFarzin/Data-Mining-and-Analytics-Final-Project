{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lws-o9jFDD6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured pypdf python-docx nltk --quiet  # Install Unstructured to extract structured content from PDFs\n",
        "\n",
        "# Download NLTK tokenizer for sentence splitting (used later)\n",
        "import nltk\n",
        "nltk.download(\"punkt\")# Required for sentence tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2IFZ0MH2RBq",
        "outputId": "50bfcafb-27d1-42ca-f48c-48615013f931"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.7/981.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and Load PDF\n",
        "\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import os\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "# Place path for PDF application\n",
        "pdf_path = \"your_application.pdf\"\n",
        "\n",
        "# Use Unstructured's partition_pdf to break the PDF into structured elements\n",
        "elements = partition_pdf(filename=pdf_path)\n",
        "\n",
        "# Preview total elements found\n",
        "print(f\"Total elements extracted: {len(elements)}\")\n",
        "print(\"First element preview:\")\n",
        "print(elements[0])"
      ],
      "metadata": {
        "id": "HavqKql6DDs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Collect all short blocks from the document\n",
        "# We're focusing on blocks with <10 words — likely to be headers, footers, or labels\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "short_blocks = [\n",
        "    el.text.strip()\n",
        "    for el in elements\n",
        "    if hasattr(el, \"text\") and 0 < len(el.text.strip().split()) < 10\n",
        "]\n",
        "\n",
        "#  Count how often each short block appears\n",
        "footer_counts = Counter(short_blocks)\n",
        "\n",
        "# If something appears too often (e.g., >5 times), it's probably a footer/header\n",
        "# You can tune this threshold depending on document size\n",
        "common_footers = {\n",
        "    text for text, count in footer_counts.items() if count > 5\n",
        "}\n",
        "\n",
        "# Optional: Inspect them\n",
        "print(\" Common footer/header candidates detected:\\n\")\n",
        "for f in list(common_footers)[:10]:\n",
        "    print(f\"- {f}\")\n"
      ],
      "metadata": {
        "id": "ddS9Oi44IlEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Define a helper function to detect bullet points\n",
        "def is_bullet_point(text):\n",
        "    \"\"\"\n",
        "    Returns True if a line looks like a bullet point.\n",
        "    Matches:\n",
        "      - • Conduct...\n",
        "      - - Maintain...\n",
        "      - * Submit...\n",
        "      - 1. Evaluate...\n",
        "      - 2) Review...\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    return bool(re.match(r\"^(\\s*[\\*\\-•]\\s+|\\d+[\\.\\)]\\s+)\", text))\n",
        "\n",
        "\n",
        "# Step 2: Filter out unwanted text\n",
        "# Keep text blocks if they are either:\n",
        "# - Long paragraphs (>15 words), OR\n",
        "# - Bullet points (even short ones)\n",
        "\n",
        "filtered_chunks = []\n",
        "discarded_chunks = []\n",
        "\n",
        "for el in elements:\n",
        "    if hasattr(el, \"text\"):\n",
        "        text = el.text.strip()\n",
        "        word_count = len(text.split())\n",
        "\n",
        "        if word_count > 15 or is_bullet_point(text):\n",
        "            filtered_chunks.append(text)\n",
        "        else:\n",
        "            discarded_chunks.append(text)\n",
        "\n",
        "# ---- Step 3: Show basic stats ----\n",
        "print(f\" Retained: {len(filtered_chunks)} meaningful paragraphs\")\n",
        "print(f\" Discarded: {len(discarded_chunks)} short or noisy blocks\")\n",
        "\n",
        "# Optional: Preview a few filtered results\n",
        "print(\"\\n Sample Clean Paragraph:\\n\")\n",
        "print(filtered_chunks[0][:500])\n",
        "\n",
        "# Optional: Preview what was discarded (debugging)\n",
        "print(\"\\n Sample Discarded Text:\\n\")\n",
        "print(discarded_chunks[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1olMyz2gc0",
        "outputId": "895ab242-c9ca-4193-9732-82c6fb2bb00f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Save cleaned output to file (optional) ===\n",
        "with open(\"clean_application.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for para in filtered_chunks:\n",
        "        f.write(para + \"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NuJTMm5eF558"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRkFfL3RF6CV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Markdown Guide",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}